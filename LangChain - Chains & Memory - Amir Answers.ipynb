{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c9318de",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— LangChain - Chains & Memory\n",
    "\n",
    "Â© Advanced Analytics, Amir Ben Haim, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7362a1",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"dotted\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29ac364",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db6da0",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>API Keys</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b23c37",
   "metadata": {},
   "source": [
    "In order to use the OpenAI language model, users are required to generate a token.\n",
    "<br></br>\n",
    "<u>Follow these simple steps to generate a token with openai:</u>\n",
    "- Go to <a href=\"url\">https://platform.openai.com/apps</a>  and signup with your email address or connect your Google Account.\n",
    "- Go to View API Keys on left side of your Personal Account Settings\n",
    "- Select Create new Secret key\n",
    "- The API access to OPENAI is a paid service\n",
    "- You have to set up billing\n",
    "- You donâ€™t need ChatGPTÂ Plus - The API and ChatGPT subscriptions are billed separately\n",
    "<br></br>\n",
    "<p style=\"background-color:Tomato\"> Make sure you read the Pricing information before experimenting</p>\n",
    "<p style=\"background-color:Tomato\">Once you add your API key, make sure to not share it with anyone! The API key should remain private</p>\n",
    "<p style=\"background-color:Tomato\">Use the <code>.env</code> file for you API key</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c38c7b",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>pip install</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af947ad",
   "metadata": {},
   "source": [
    "```powershell\n",
    "pip install langchain langchain-openai langchain-community\n",
    "pip install python-dotenv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999405c3",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>API Key Setup</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23adb04f",
   "metadata": {},
   "source": [
    "Before using LangChain with OpenAI, set your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474e8c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # Loads variables from .env\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "print(openai_key[:5])  # Just to check, don't print the full key!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a286dd59",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"dotted\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a0abc",
   "metadata": {},
   "source": [
    "## Initilize SETTINGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebdade7",
   "metadata": {},
   "source": [
    "```python\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-2024-11-20\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496fc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.memory import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-2024-11-20\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fff9fb",
   "metadata": {},
   "source": [
    "## 1. Minimal Stateless Chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c9cea3",
   "metadata": {},
   "source": [
    "```python\n",
    "response = chain.invoke({\"question\": \"What is LangChain? very short answer\"})\n",
    "print(response.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb34d08",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o-2024-11-20\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "chain = prompt | llm\n",
    "\n",
    "response = chain.invoke({\"question\": \"What is LangChain? very short answer\"})\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9c504",
   "metadata": {},
   "source": [
    "## 2. Prompt Template Error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6fd68d",
   "metadata": {},
   "source": [
    "```python\n",
    "try:\n",
    "    response = chain.invoke({\"xxx\": \"What is LangChain?\"})\n",
    "    print(response.content)\n",
    "except...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733242b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    response = chain.invoke({\"xxx\": \"What is LangChain?\"})\n",
    "    print(response.content)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405f33fe",
   "metadata": {},
   "source": [
    "## 3. Adding a String Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dded6d4e",
   "metadata": {},
   "source": [
    "```python\n",
    "response = chain.invoke({\"question\": \"What is LangChain? very short answer\"})\n",
    "print(response)\n",
    "print(type(response)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ee67e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | StrOutputParser()\n",
    "response = chain.invoke({\"question\": \"What is LangChain? very short answer\"})\n",
    "print(response)\n",
    "print(type(response)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a8df6d",
   "metadata": {},
   "source": [
    "## 4. Using a JSON Output Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec8babd",
   "metadata": {},
   "source": [
    "```python\n",
    "response = chain.invoke({\"question\": \"Where is Tel Aviv located?\"})\n",
    "print(response)\n",
    "print(type(response)) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855e05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "parser = JsonOutputParser()\n",
    "\n",
    "prompt_json = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that gives very short answers - you answer the questions in a format of a json, first key as city, second key is country\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt_json | llm | parser\n",
    "\n",
    "response = chain.invoke({\"question\": \"Where is Tel Aviv located?\"})\n",
    "print(response)\n",
    "print(type(response)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c1d7e",
   "metadata": {},
   "source": [
    "## 5. Sequential Chain â€” Two Step Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d4258",
   "metadata": {},
   "source": [
    "```python\n",
    "def sequential_chain(field):\n",
    "    subjects = (prompt1 | llm | parser).invoke({\"field\": field})\n",
    "    summary = (prompt2 | llm | parser).invoke({\"subjects\": subjects})\n",
    "    return summary\n",
    "\n",
    "print(sequential_chain(\"Math\"))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185cb37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = StrOutputParser()\n",
    "prompt1 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"List three major subjects in a certain field {field}, separated by commas.\"),\n",
    "    (\"user\", \"{field}\")\n",
    "])\n",
    "prompt2 = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize these subjects in one sentence: {subjects}\")\n",
    "])\n",
    "\n",
    "def sequential_chain(field):\n",
    "    subjects = (prompt1 | llm | parser).invoke({\"field\": field})\n",
    "    summary = (prompt2 | llm | parser).invoke({\"subjects\": subjects})\n",
    "    return summary\n",
    "\n",
    "print(sequential_chain(\"Math\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1850012b",
   "metadata": {},
   "source": [
    "## 6. Chain with a Custom Python Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba4bee",
   "metadata": {},
   "source": [
    "Use `sequential_chain()` from Exercise 5, and add to it if necessary\n",
    "<br></br>\n",
    "\n",
    "```python\n",
    "    sequential_chain_with_log('history')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95920b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_chain_with_log(field):\n",
    "    summary = sequential_chain(field)\n",
    "    with open(f'{field}.txt', 'w+') as f:\n",
    "        f.write(summary)\n",
    "        print(f'a .txt file with summary about {field} has been created')\n",
    "\n",
    "\n",
    "sequential_chain_with_log('history')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0627aac7",
   "metadata": {},
   "source": [
    "## 7. Basic Stateful Chain with Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166c86ff",
   "metadata": {},
   "source": [
    "```python\n",
    "session_id = \"user1\"\n",
    "output1 = chat_chain.invoke({\"input\": \"My name is Sara and I love coffee.\"}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(output1.content)\n",
    "output2 = chat_chain.invoke({\"input\": \"What is my name and favorite drink?\"}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(output2.content)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347665b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LLM and prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that gives very short answers\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_history(session_id):\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "\n",
    "chat_chain = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history=get_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"history\",\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "session_id = \"user1\"\n",
    "output1 = chat_chain.invoke({\"input\": \"My name is Sara and I love coffee.\"}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(output1.content)\n",
    "output2 = chat_chain.invoke({\"input\": \"What is my name and favorite drink?\"}, config={\"configurable\": {\"session_id\": session_id}})\n",
    "print(output2.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02aa3ed4",
   "metadata": {},
   "source": [
    "## 8. Inspect the Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9894f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store[session_id].messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03642c5c",
   "metadata": {},
   "source": [
    "## 9. Resetting a SPECIFIC Chat Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93bbda",
   "metadata": {},
   "source": [
    "Build a `def` function\n",
    "<br></br>\n",
    "\n",
    "```python\n",
    "reset_session(session_id)\n",
    "print(store[session_id].messages)\n",
    "print(store)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd62379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session(session_id):\n",
    "    store[session_id] = ChatMessageHistory()\n",
    "\n",
    "reset_session(session_id)\n",
    "print(store[session_id].messages)\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e66ba4",
   "metadata": {},
   "source": [
    "## 10. Resetting `store`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08d483e",
   "metadata": {},
   "source": [
    "```python\n",
    "print(store)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79fc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = {}\n",
    "\n",
    "print(store)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba9cf94",
   "metadata": {},
   "source": [
    "## 11. Multi-Session Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006955fc",
   "metadata": {},
   "source": [
    "Build a `def` function\n",
    "<br></br>\n",
    "\n",
    "```python\n",
    "session_a = \"userA\"\n",
    "chat_chain.invoke({\"input\": \"I am Alex, 25.\"}, config={\"configurable\": {\"session_id\": session_a}})\n",
    "chat_chain.invoke({\"input\": \"How old am I?\"}, config={\"configurable\": {\"session_id\": session_a}})\n",
    "\n",
    "session_b = \"userB\"\n",
    "chat_chain.invoke({\"input\": \"I am Dana, 35.\"}, config={\"configurable\": {\"session_id\": session_b}})\n",
    "chat_chain.invoke({\"input\": \"What is my name?\"}, config={\"configurable\": {\"session_id\": session_b}})\n",
    "\n",
    "\n",
    "print_store_chats()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c91c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_store_chats():\n",
    "    for k,v in store.items():\n",
    "        print(f'session_id: {k}')\n",
    "        print(len(f'session_id: {k})') * '-')\n",
    "        print(store[k])\n",
    "        print('\\n')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "session_a = \"userA\"\n",
    "chat_chain.invoke({\"input\": \"I am Alex, 25.\"}, config={\"configurable\": {\"session_id\": session_a}})\n",
    "chat_chain.invoke({\"input\": \"How old am I?\"}, config={\"configurable\": {\"session_id\": session_a}})\n",
    "\n",
    "session_b = \"userB\"\n",
    "chat_chain.invoke({\"input\": \"I am Dana, 35.\"}, config={\"configurable\": {\"session_id\": session_b}})\n",
    "chat_chain.invoke({\"input\": \"What is my name?\"}, config={\"configurable\": {\"session_id\": session_b}})\n",
    "\n",
    "\n",
    "print_store_chats()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
