{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2af74cc",
   "metadata": {
    "id": "b2af74cc"
   },
   "source": [
    "# Model Optimization\n",
    "\n",
    "© Advanced Analytics, Amir Ben Haim, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874df1e9",
   "metadata": {},
   "source": [
    "<font color=\"yellow\">UPDATED</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7d854d",
   "metadata": {
    "id": "db7d854d"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"dotted\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38265701",
   "metadata": {
    "id": "38265701"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297772a3",
   "metadata": {
    "id": "297772a3"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>Resetting OpenAI API **EVALS & FINE-TUNING**</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3386f2c3",
   "metadata": {
    "id": "3386f2c3"
   },
   "source": [
    "<p style=\"background-color:blue; font-size:30px; color:yellow\"> It's easier to follow the notebook if you reset (delete) OpenAI API <b>FILES & EVALS & FINE-TUNING</b>\n",
    "<br>Use at your own discretion</p>\n",
    "\n",
    "[OpenAI Storage](https://platform.openai.com/storage)\n",
    "<br>\n",
    "[OpenAI Evals](https://platform.openai.com/evaluations?tab=evaluations)\n",
    "<br>\n",
    "[OpenAI Eval Runs](https://platform.openai.com/evaluations?tab=runs)\n",
    "<br>\n",
    "[OpenAI Fine-tuning](https://platform.openai.com/finetune)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254fd73e",
   "metadata": {
    "id": "254fd73e"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>API Keys</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d440b",
   "metadata": {
    "id": "3a3d440b"
   },
   "source": [
    "In order to use the OpenAI language model, users are required to generate a token.\n",
    "<br></br>\n",
    "<u>Follow these simple steps to generate a token with openai:</u>\n",
    "- Go to <a href=\"url\">https://platform.openai.com/apps</a>  and signup with your email address or connect your Google Account.\n",
    "- Go to View API Keys on left side of your Personal Account Settings\n",
    "- Select Create new Secret key\n",
    "- The API access to OPENAI is a paid service\n",
    "- You have to set up billing\n",
    "- You don’t need ChatGPT Plus - The API and ChatGPT subscriptions are billed separately\n",
    "<br></br>\n",
    "<p style=\"background-color:Tomato\"> Make sure you read the Pricing information before experimenting</p>\n",
    "<p style=\"background-color:Tomato\">Once you add your API key, make sure to not share it with anyone! The API key should remain private</p>\n",
    "<p style=\"background-color:Tomato\">Use the <code>.env</code> file for you API key</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637d306e",
   "metadata": {
    "id": "637d306e"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>pip install</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64a025c",
   "metadata": {
    "id": "a64a025c"
   },
   "source": [
    "```powershell\n",
    "pip install openai\n",
    "pip install python-dotenv\n",
    "pip install scikit-learn\n",
    "pip install pandas\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8e8d5",
   "metadata": {
    "id": "ace8e8d5"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>API Key Setup</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774f7b02",
   "metadata": {
    "id": "774f7b02"
   },
   "source": [
    "Before using LangChain with OpenAI, set your API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "017288ce",
   "metadata": {
    "id": "017288ce"
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Loads variables from .env\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd98b51d",
   "metadata": {
    "id": "cd98b51d"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<hr class=\"dotted\">\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe066493",
   "metadata": {
    "id": "fe066493"
   },
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d204b3",
   "metadata": {
    "id": "27d204b3"
   },
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>Create an eval for a task</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea88860",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 1 - Describe a task to be done by a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788f7b21",
   "metadata": {
    "id": "788f7b21"
   },
   "source": [
    "In the example below we want the model <u>classify Hotel Reviews</u>\n",
    "> **Goal**: Classify customer review as `Positive`, `Negative`, or `Neutral`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c5a0c7",
   "metadata": {
    "id": "36c5a0c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You are an expert in categorizing  Hotel Guest Complaints. Given the complaint\n",
    "below, categorize the complaint into one of \"Positive\", \"Negative\", \"Neutral\". Respond with only one of those words.\n",
    "\"\"\"\n",
    "\n",
    "complaint = \"Had a great time. Everything from the service to the bed was perfect\"\n",
    "\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"developer\", \"content\": instructions},\n",
    "        {\"role\": \"user\", \"content\": complaint}\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1768b17",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 2 - Create an eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c29695",
   "metadata": {},
   "source": [
    "Let's set up an eval to test this behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "789978b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EvalCreateResponse(id='eval_68582dfa15388191a483ca7bbdf10b90', created_at=1750609402, data_source_config=EvalCustomDataSourceConfig(schema_={'type': 'object', 'properties': {'item': {'type': 'object', 'properties': {'hotel_review': {'type': 'string'}, 'correct_label': {'type': 'string'}}, 'required': ['hotel_review', 'correct_label']}, 'sample': {'type': 'object', 'properties': {'model': {'type': 'string'}, 'choices': {'type': 'array', 'items': {'type': 'object', 'properties': {'message': {'type': 'object', 'properties': {'role': {'type': 'string', 'enum': ['assistant']}, 'content': {'type': ['string', 'null']}, 'refusal': {'type': ['boolean', 'null']}, 'tool_calls': {'type': ['array', 'null'], 'items': {'type': 'object', 'properties': {'type': {'type': 'string', 'enum': ['function']}, 'function': {'type': 'object', 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'string'}}, 'required': ['name', 'arguments']}, 'id': {'type': 'string'}}, 'required': ['type', 'function', 'id']}}, 'function_call': {'type': ['object', 'null'], 'properties': {'name': {'type': 'string'}, 'arguments': {'type': 'string'}}, 'required': ['name', 'arguments']}}, 'required': ['role']}, 'finish_reason': {'type': 'string'}}, 'required': ['index', 'message', 'finish_reason']}}, 'output_text': {'type': 'string'}, 'output_json': {'type': 'object'}, 'output_tools': {'type': 'array', 'items': {'type': 'object'}}, 'output_reasoning_summary': {'type': ['string', 'null']}, 'input_tools': {'type': 'array', 'items': {'type': 'object'}}}, 'required': ['model', 'choices']}}, 'required': ['item', 'sample']}, type='custom'), metadata={}, name='Hotel Guests Complaint Routing', object='eval', testing_criteria=[StringCheckGrader(input='{{ sample.output_text }}', name='Match output to human label', operation='eq', reference='{{ item.correct_label }}', type='string_check', id='Match output to human label-cc4a9cbb-1b61-4f6d-8974-215b3dd6bf16')])\n",
      "eval_68582dfa15388191a483ca7bbdf10b90\n"
     ]
    }
   ],
   "source": [
    "eval_obj = client.evals.create(\n",
    "\n",
    "    name=\"Hotel Guests Complaint Routing\",\n",
    "\n",
    "    data_source_config={\n",
    "        \"type\": \"custom\",\n",
    "        \"item_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"hotel_review\": {\"type\": \"string\"},\n",
    "                \"correct_label\": {\"type\": \"string\"},\n",
    "            },\n",
    "            \"required\": [\"hotel_review\", \"correct_label\"],\n",
    "        },\n",
    "        \"include_sample_schema\": True,\n",
    "\n",
    "    },\n",
    "    testing_criteria=[\n",
    "        {\n",
    "            \"type\": \"string_check\",\n",
    "            \"name\": \"Match output to human label\",\n",
    "            \"input\": \"{{ sample.output_text }}\",\n",
    "            \"operation\": \"eq\",\n",
    "            \"reference\": \"{{ item.correct_label }}\",\n",
    "        }\n",
    "    ],\n",
    ")\n",
    "\n",
    "\n",
    "print(eval_obj)\n",
    "print(eval_obj.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4a0574",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>Test a prompt with your eval</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac36c92",
   "metadata": {},
   "source": [
    "We've created an eval that describes the desired behavior of our application, let's test a prompt with a set of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc74c834",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 3 - Uploading test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccadd1",
   "metadata": {},
   "source": [
    "- Use the  **JSONL** file `hotel_review_sentiment_test.jsonl`\n",
    "\n",
    "- Now, upload the test data file to the OpenAI platform so we can reference it later [OpenAI Storage](https://platform.openai.com/storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "affd8d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FileObject(id='file-9U3oiMqWpGEwvcWNHJ4aZu', bytes=10450, created_at=1750609497, filename='hotel_review_sentiment_test.jsonl', object='file', purpose='evals', status='processed', expires_at=None, status_details=None)\n",
      "file-9U3oiMqWpGEwvcWNHJ4aZu\n"
     ]
    }
   ],
   "source": [
    "file = client.files.create(\n",
    "    file=open(\"hotel_review_sentiment_test.jsonl\", \"rb\"),\n",
    "    purpose=\"evals\"\n",
    ")\n",
    "\n",
    "\n",
    "print(file)\n",
    "print(file.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cfde7b",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<p style=\"background-color:blue; font-size:20px; color:yellow\"> Very Important!</b></p>\n",
    "\n",
    "To evaluate the effectiveness of model fine-tuning, <u>**I deliberately included the made-up word \"xxx\" in several complaint examples**.</u>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2076e320",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 4 - Creating an eval run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93facf",
   "metadata": {},
   "source": [
    "With our test data in place, let's evaluate a prompt and see how it performs against our test criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4bab673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunCreateResponse(id='evalrun_68582ebf20648191bcc89eda10c732f9', created_at=1750609599, data_source=CreateEvalCompletionsRunDataSource(source=SourceFileID(id='file-9U3oiMqWpGEwvcWNHJ4aZu', type='file_id'), type='completions', input_messages=InputMessagesTemplate(template=[InputMessagesTemplateTemplateMessage(content=ResponseInputText(text='\\nYou are an expert in categorizing  Hotel Guest Complaints. Given the complaint\\nbelow, categorize the complaint into one of \"Positive\", \"Negative\", \"Neutral\". Respond with only one of those words.\\n', type='input_text'), role='developer', type='message'), InputMessagesTemplateTemplateMessage(content=ResponseInputText(text='{{ item.hotel_review }}', type='input_text'), role='user', type='message')], type='template'), model='gpt-4.1', sampling_params=None), error=None, eval_id='eval_68582dfa15388191a483ca7bbdf10b90', metadata={}, model='gpt-4.1', name='Categorization text run', object='eval.run', per_model_usage=None, per_testing_criteria_results=None, report_url='https://platform.openai.com/evaluations/eval_68582dfa15388191a483ca7bbdf10b90?project_id=proj_hmIydh3znfyleGOYdvTLvb84&run_id=evalrun_68582ebf20648191bcc89eda10c732f9', result_counts=ResultCounts(errored=0, failed=0, passed=0, total=0), status='queued', shared_with_openai=False)\n",
      "evalrun_68582ebf20648191bcc89eda10c732f9\n"
     ]
    }
   ],
   "source": [
    "instructions = \"\"\"\n",
    "You are an expert in categorizing  Hotel Guest Complaints. Given the complaint\n",
    "below, categorize the complaint into one of \"Positive\", \"Negative\", \"Neutral\". Respond with only one of those words.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "run = client.evals.runs.create(\n",
    "\n",
    "    eval_obj.id, # YOUR_EVAL_ID\n",
    "\n",
    "    name=\"Categorization text run\",\n",
    "\n",
    "    data_source={\n",
    "        \"type\": \"completions\",\n",
    "        \"model\": \"gpt-4.1\",\n",
    "        \"input_messages\": {\n",
    "            \"type\": \"template\",\n",
    "            \"template\": [\n",
    "                {\"role\": \"developer\", \"content\": instructions},\n",
    "                {\"role\": \"user\", \"content\": \"{{ item.hotel_review }}\"},\n",
    "            ],\n",
    "        },\n",
    "        \"source\": {\"type\": \"file_id\", \"id\": file.id}, # YOUR_FILE_ID\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(run)\n",
    "print(run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f3fc3c",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "### <u>Analyze the results</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e657b9c",
   "metadata": {},
   "source": [
    "We've created an eval that describes the desired behavior of our application, let's test a prompt with a set of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f6dc9",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 5 - Run has now been queued"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b42497",
   "metadata": {},
   "source": [
    "Fetch the current status of an eval run via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c3d2f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunRetrieveResponse(id='evalrun_68582ebf20648191bcc89eda10c732f9', created_at=1750609599, data_source=CreateEvalCompletionsRunDataSource(source=SourceFileID(id='file-9U3oiMqWpGEwvcWNHJ4aZu', type='file_id'), type='completions', input_messages=InputMessagesTemplate(template=[InputMessagesTemplateTemplateMessage(content=ResponseInputText(text='\\nYou are an expert in categorizing  Hotel Guest Complaints. Given the complaint\\nbelow, categorize the complaint into one of \"Positive\", \"Negative\", \"Neutral\". Respond with only one of those words.\\n', type='input_text'), role='developer', type='message'), InputMessagesTemplateTemplateMessage(content=ResponseInputText(text='{{ item.hotel_review }}', type='input_text'), role='user', type='message')], type='template'), model='gpt-4.1', sampling_params=None), error=None, eval_id='eval_68582dfa15388191a483ca7bbdf10b90', metadata={}, model='gpt-4.1', name='Categorization text run', object='eval.run', per_model_usage=[PerModelUsage(cached_tokens=0, completion_tokens=105, invocation_count=105, run_model_name='gpt-4.1-2025-04-14', prompt_tokens=6561, total_tokens=6666, model_name='gpt-4.1-2025-04-14')], per_testing_criteria_results=[PerTestingCriteriaResult(failed=8, passed=97, testing_criteria='Match output to human label-cc4a9cbb-1b61-4f6d-8974-215b3dd6bf16')], report_url='https://platform.openai.com/evaluations/eval_68582dfa15388191a483ca7bbdf10b90?project_id=proj_hmIydh3znfyleGOYdvTLvb84&run_id=evalrun_68582ebf20648191bcc89eda10c732f9', result_counts=ResultCounts(errored=0, failed=8, passed=97, total=105), status='completed', shared_with_openai=False)\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "run_retrieve = client.evals.runs.retrieve(\n",
    "    eval_id=eval_obj.id, # YOUR_EVAL_ID\n",
    "    run_id=run.id # YOUR_RUN_ID\n",
    "    )\n",
    "\n",
    "\n",
    "print(run_retrieve)\n",
    "print(run_retrieve.status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a532d51",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "####  Exercise 6 - Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21424522",
   "metadata": {},
   "source": [
    "Return the API response as the folowing format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ddc902e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed\n",
      "gpt-4.1\n",
      "Categorization text run\n",
      "1750609599\n",
      "2025-06-22 19:26:39\n",
      "ResultCounts(errored=0, failed=8, passed=97, total=105)\n"
     ]
    }
   ],
   "source": [
    "print(run_retrieve.status)\n",
    "print(run_retrieve.model)\n",
    "print(run_retrieve.name)\n",
    "from datetime import datetime\n",
    "timestamp = run_retrieve.created_at\n",
    "readable = datetime.fromtimestamp(timestamp)\n",
    "print(timestamp)\n",
    "print(readable)\n",
    "print(run_retrieve.result_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2c6824",
   "metadata": {},
   "source": [
    "<br></br>\n",
    "\n",
    "#### Exercise 7 - Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ac41b0",
   "metadata": {},
   "source": [
    "Fetch the raw evaluation items and compute the folowing metrics yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "085e8eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "--------\n",
      "92.38%\n"
     ]
    }
   ],
   "source": [
    "passed = run_retrieve.result_counts.passed\n",
    "total = run_retrieve.result_counts.total\n",
    "\n",
    "print('Accuracy')\n",
    "print(len('Accuracy')*'-')\n",
    "print(f'{(passed / total) :.2%}')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
